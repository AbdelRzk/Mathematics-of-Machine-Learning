{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea28c81",
   "metadata": {},
   "source": [
    "## Motivation and Theory of Projection.\n",
    "### Motivation.\n",
    "\n",
    "Projections are an important class of linear transformation and play an important role in Machine Learning (ML). In ML, we often deal with data that are high-dimensional. High-dimensional data are hard to analyse or to visualize. However high-dimensional data quite often possesses the property that only few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. More precisely we can project a high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional to learn more about the data and extract relevant patterns.\n",
    "\t\t\t\n",
    "We will focus on the Orthogonal Projection and as inner product, we will use the dot-product defined as : for $y = [y_1, y_2,..., y_n]^T$ and $z = [z_1, z_2,..., z_n]^T$ in $\\mathbb{R}^n$,\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\langle y,z \\rangle = y^{T} z = \\sum_{i=1}^{n} y_i z_i.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\n",
    "Let $V$ a vector space and $U \\subseteq V$ a subspace of $V$. A projection from $V$ to $U$ is a linear mapping $\\Phi_{U} : V \\to U$ such that $\\Phi_{U} \\circ \\Phi_{U} = \\Phi_{U}^2 = \\Phi_{U}$. The projection matrix associated to $\\Phi_{U}$ is denoted by $P_{\\Phi}$ and satisfies the same property as $\\Phi$ which is : $P_{\\Phi} \\circ P_{\\Phi} = P_{\\Phi}^2 = P_{\\Phi}$.\n",
    "\t\t\t\n",
    "### Theory\n",
    "\n",
    "Let $V$ a vector space with dimension $n$. In this work, we will assume that $V = \\mathbb{R}^{n}$. Let $U$ a subspace of $V$ of dimension $m$ and $\\{ b_1, b_2,..., b_m\\}$ a basis of $U$, where for all $i=1,...,m$, $b_i \\in V$. Let $x \\in V$ a high-dimensional data and $P_{\\Phi}(x)$ its projection onto $U$. The goal is to project $x$ from the high-dimensional $n$ to the lower-dimensional $m$, ($m < n$). We are looking for $\\lambda_1, \\lambda_2,..,\\lambda_m \\in \\mathbb{R}$ such that :\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\Phi_{U}(x) = \\sum_{i=1}^{m} \\lambda_i b_i.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\n",
    "When projecting $x$ onto $U$, we are looking for the vector $\\Phi_{U}(x)$ that is closest to $x$. So the quantity $\\|x - \\Phi_{U}(x)\\|$ should be minimal, which implies that the vector $x - \\Phi_{U}(x)$ should be orthogonal to $U$, that is for all $i=1,...,m$ :\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\langle b_i, x - \\Phi_{U}(x) \\rangle = 0.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\n",
    "Let $B = [b_1\\,\\, b_2\\,\\, ...\\,\\, b_m] \\in \\mathbb{R}^{n\\times m}$ be the matrix whose columns are vectors of the basis $\\{ b_1, b_2,..., b_m\\}$, $ \\lambda = [\\lambda_1, \\lambda_2,..,\\lambda_m ]^T$. Using the dot-product as inner product, for all $i = 1,...,m$, we have:\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\langle b_i, x - \\Phi_{U}(x) \\rangle &= b_{i}^{T}(x - \\Phi_{U}(x)) = 0, \\text{ then } B^T(x - \\Phi_{U}(x)) = 0,\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\twe have\n",
    "$$\\Phi_{U}(x) = \\sum_{i=1}^{m} \\lambda_i b_i = [b_1\\,\\, b_2\\,\\, ...\\,\\, b_m] [\\lambda_1\\,\\,  \\lambda_2\\,\\, ...\\,\\, \\lambda_m ]^T = B \\lambda$$\n",
    "\t\t\tthen\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\tB^T(x - \\Phi_{U}(x)) = B^T(x - B \\lambda) = 0, \\text{ which implies } B^T B \\lambda = B^T x\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\tthus\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\lambda = (B^T B)^{-1}B^T x,\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\ttherefore\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t\\Phi_{U}(x) = B \\lambda = B (B^T B)^{-1}B^T x.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\tThe projection matrix is defined by :\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\tP_{\\Phi} =  B (B^T B)^{-1}B^T.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\tWe can also write $\\Phi_{U}(x) = \\Phi_{span\\{ b_1, b_2,..., b_m\\}}(x)$, where $\\{ b_1, b_2,..., b_m\\}$ is a basis of $U$.\n",
    "\n",
    "### Application: Gram-Schmidt Orthogonalization\n",
    "\n",
    "It transforms any basis $\\{ b_1, b_2,..., b_n\\}$ of $n$-dimensional vector space $V$ into an orthogonal/orthonormal basis $\\{ u_1, u_2,..., u_m\\}$ of $V$, where :\n",
    "\t\t\t\\begin{align*}\n",
    "\t\t\t\t& u_1 = b_1\n",
    "\t\t\t\t\\\\& u_k = b_k - \\Phi_{span\\{ u_1, u_2,..., u_{k-1}\\}}(b_k),\\,\\,\\, k = 2,\\,3,\\,...,\\, n.\n",
    "\t\t\t\\end{align*}\n",
    "\t\t\tNext we are going to implement Gram-Schmidt Orthogonalization on python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c35a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a80fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectionMatrix(B):\n",
    "    a, b = B.shape\n",
    "    if a==1:\n",
    "        B = np.transpose(B)\n",
    "        return np.dot(B,np.dot(np.linalg.inv(np.dot(np.transpose(B), B)), np.transpose(B)))\n",
    "    return np.dot(B,np.dot(np.linalg.inv(np.dot(np.transpose(B), B)), np.transpose(B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b236a6e",
   "metadata": {},
   "source": [
    "#### Example of application for the projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97babe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.22222222, 0.22222222],\n",
       "       [0.22222222, 0.44444444, 0.44444444],\n",
       "       [0.22222222, 0.44444444, 0.44444444]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1,2,2]])\n",
    "projectionMatrix(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e8ef",
   "metadata": {},
   "source": [
    "#### Projection of a vector x = [1,1,1] onto a line spanned by B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce320c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55555556],\n",
       "       [1.11111111],\n",
       "       [1.11111111]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.dot(projectionMatrix(B), np.transpose(np.array([[1,1,1]])))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b9dda",
   "metadata": {},
   "source": [
    "#### Projection matrix : Case of a plan spanned by b1 = [1,1,1] and b2 = [0,1,2]. We stacked those two vector in a matrix as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed2165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83333333,  0.33333333, -0.16666667],\n",
       "       [ 0.33333333,  0.33333333,  0.33333333],\n",
       "       [-0.16666667,  0.33333333,  0.83333333]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectionMatrix(np.array([[1,0],[1,1],[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5554e3d",
   "metadata": {},
   "source": [
    "#### We will use the projection function defined above to code the Gram-Schmidt orthogonalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74d2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GramSchmidthOrthogonalisation(B):\n",
    "    U = np.zeros(B.shape)\n",
    "    n, m = B.shape\n",
    "    U[:,0] = B[:,0]\n",
    "    V = np.transpose(np.array([B[:,0]]))\n",
    "    \n",
    "    for i in range(1,m):\n",
    "        U[:,i] = (np.transpose(np.array([B[:,i]])) - np.dot(projectionMatrix(V), np.transpose(np.array([B[:,i]]))))[:,0]\n",
    "        V = np.hstack((V,np.transpose(np.array([U[:,i]]))))\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff5c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1. -1.]\n",
      " [ 1.  0.  2.]\n",
      " [ 1.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "C = np.array([[1,0,1],[1,1,3],[1,2,-1]])\n",
    "U = GramSchmidthOrthogonalisation(C)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb6a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
